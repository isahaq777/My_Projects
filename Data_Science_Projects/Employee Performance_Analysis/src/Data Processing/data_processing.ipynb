{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_excel(\"E:\\Data_mites_projects\\Employee Performance Analysis\\INX_Future_Inc_Employee_Performance_CDS_Project2_Data_V1.8 (1).xls\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "column = df.columns \n",
    "for i in column:\n",
    "    print(df.dtypes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Checking for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isnull() for Standard Missing Values:\n",
    "\n",
    "Identifies standard representations of missing values such as NaN, None, and null.\n",
    "Custom Missing Value Check:\n",
    "\n",
    "Searches for additional placeholders for missing values (e.g., \"NA\", \"N/A\", \"null\", \" \").\n",
    "Combine Results:\n",
    "\n",
    "Combines the counts of standard and custom missing values to calculate the total missing count.\n",
    "Output:\n",
    "\n",
    "Displays a summary showing the count of missing values (both standard and additional) for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_missing_values(data):\n",
    "    # Standard missing values\n",
    "    missing_count = data.isnull().sum()\n",
    "    \n",
    "    # Check for other representations of missing values\n",
    "    other_missing_values = [\"\", \" \", \"NA\", \"N/A\", \"n/a\", \"null\", \"NULL\", \"NaN\", \"nan\", \"-\", \"--\"]\n",
    "    \n",
    "    additional_missing = pd.DataFrame({\n",
    "        col: data[col].apply(lambda x: x in other_missing_values).sum()\n",
    "        for col in data.columns\n",
    "    }, index=[\"Other Missing Count\"]).T\n",
    "    \n",
    "    # Combine both checks\n",
    "    total_missing = pd.DataFrame({\n",
    "        \"Standard Missing Count\": missing_count,\n",
    "        \"Other Missing Count\": additional_missing[\"Other Missing Count\"],\n",
    "        \"Total Missing Count\": missing_count + additional_missing[\"Other Missing Count\"]\n",
    "    })\n",
    "    \n",
    "    print(\"Summary of Missing Values:\")\n",
    "    print(total_missing)\n",
    "    return total_missing\n",
    "\n",
    "# Call the function on your dataset\n",
    "missing_summary = check_missing_values(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(data.describe(include='all'))  # Summary of numeric and categorical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#removing the unwanted column \n",
    "data = data.drop(['EmpNumber'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    unique_values = data[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# List of categorical columns\n",
    "binary_columns = [\"Gender\", \"OverTime\", \"Attrition\"]  # Binary encoding\n",
    "label_columns = [\"EducationBackground\", \"MaritalStatus\", \"EmpDepartment\", \"BusinessTravelFrequency\"]  # Label encoding\n",
    "onehot_columns = [\"EmpJobRole\"]  # One-hot encoding\n",
    "\n",
    "# Binary encoding: Convert Yes/No and Male/Female to 0/1\n",
    "binary_map = {\"Male\": 0, \"Female\": 1, \"No\": 0, \"Yes\": 1}\n",
    "for col in binary_columns:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].map(binary_map)\n",
    "\n",
    "# Label encoding for categorical variables with a few unique values\n",
    "label_encoders = {}\n",
    "for col in label_columns:\n",
    "    if col in data.columns:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        data[col] = label_encoders[col].fit_transform(data[col])\n",
    "\n",
    "# One-hot encoding for categorical variables with many unique values\n",
    "data = pd.get_dummies(data, columns=onehot_columns, drop_first=True)  # Drop first category to avoid multicollinearity\n",
    "\n",
    "# Final check\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    unique_values = data[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = data.astype(int)  # Convert all boolean columns to 0/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for col in [\"Gender\", \"OverTime\", \"Attrition\"]:\n",
    "    print(f\"{col}: {data[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    unique_values = data[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]  # Filter only missing columns\n",
    "print(\"Missing Values Count:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identify numerical columns\n",
    "num_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculate IQR and detect outliers\n",
    "outlier_summary = {}\n",
    "for col in num_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col]\n",
    "    outlier_summary[col] = len(outliers)\n",
    "\n",
    "# Print columns with outliers\n",
    "outlier_summary = {k: v for k, v in outlier_summary.items() if v > 0}\n",
    "print(\"Columns with outliers:\", outlier_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers(df, columns):\n",
    "    outlier_info = {}\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)  # First quartile (25th percentile)\n",
    "        Q3 = df[col].quantile(0.75)  # Third quartile (75th percentile)\n",
    "        IQR = Q3 - Q1                # Interquartile Range\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Find outlier indexes\n",
    "        outlier_indexes = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()\n",
    "\n",
    "        if outlier_indexes:\n",
    "            outlier_info[col] = outlier_indexes  # Store indexes\n",
    "\n",
    "    return outlier_info\n",
    "\n",
    "\n",
    "numerical_columns = ['Age', 'Gender', 'EducationBackground', 'MaritalStatus',\n",
    "       'EmpDepartment', 'BusinessTravelFrequency', 'DistanceFromHome',\n",
    "       'EmpEducationLevel', 'EmpEnvironmentSatisfaction', 'EmpHourlyRate',\n",
    "       'EmpJobInvolvement', 'EmpJobLevel', 'EmpJobSatisfaction',\n",
    "       'NumCompaniesWorked', 'OverTime', 'EmpLastSalaryHikePercent',\n",
    "       'EmpRelationshipSatisfaction', 'TotalWorkExperienceInYears',\n",
    "       'TrainingTimesLastYear', 'EmpWorkLifeBalance',\n",
    "       'ExperienceYearsAtThisCompany', 'ExperienceYearsInCurrentRole',\n",
    "       'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Attrition',\n",
    "       'PerformanceRating', 'EmpJobRole_Data Scientist',\n",
    "       'EmpJobRole_Delivery Manager', 'EmpJobRole_Developer',\n",
    "       'EmpJobRole_Finance Manager', 'EmpJobRole_Healthcare Representative',\n",
    "       'EmpJobRole_Human Resources', 'EmpJobRole_Laboratory Technician',\n",
    "       'EmpJobRole_Manager', 'EmpJobRole_Manager R&D',\n",
    "       'EmpJobRole_Manufacturing Director', 'EmpJobRole_Research Director',\n",
    "       'EmpJobRole_Research Scientist', 'EmpJobRole_Sales Executive',\n",
    "       'EmpJobRole_Sales Representative', 'EmpJobRole_Senior Developer',\n",
    "       'EmpJobRole_Senior Manager R&D', 'EmpJobRole_Technical Architect',\n",
    "       'EmpJobRole_Technical Lead']\n",
    "\n",
    "# Detect outliers\n",
    "outliers_dict = detect_outliers(data, numerical_columns)\n",
    "\n",
    "# Print outlier details\n",
    "print(\"ðŸ“Œ Outliers detected in the following columns:\\n\")\n",
    "for col, indexes in outliers_dict.items():\n",
    "    print(f\"ðŸ”¹ Column: {col} | Outlier Count: {len(indexes)} | Positions: {indexes[:10]}...\")  # Show first 10 positions\n",
    "\n",
    "# Create a DataFrame of only outlier rows for analysis\n",
    "outlier_rows = data.loc[np.unique(sum(outliers_dict.values(), []))]\n",
    "\n",
    "print(\"\\nðŸ›  Sample of Outlier Rows:\\n\", outlier_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Get correlation of all features with the target column 'PerformanceRating'\n",
    "target_corr = correlation_matrix[\"PerformanceRating\"].sort_values(ascending=False)\n",
    "\n",
    "# Display the top positively and negatively correlated features\n",
    "print(\"ðŸ”¹ Features with Highest Correlation to PerformanceRating:\\n\", target_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# STEP 0: Outlier Detection which was already done\n",
    "##############################\n",
    "\n",
    "# For multivariate outlier detection, we use Mahalanobis Distance.\n",
    "# Here, we select the numerical columns for this calculation.\n",
    "#below are the columns which are selected for the outlier detection,based on the correlation matrix and market analysis.\n",
    "numeric_cols = [\n",
    "    'NumCompaniesWorked', 'TotalWorkExperienceInYears', 'TrainingTimesLastYear',\n",
    "    'ExperienceYearsAtThisCompany', 'ExperienceYearsInCurrentRole','EmpEnvironmentSatisfaction','EmpLastSalaryHikePercent', \n",
    "    'YearsSinceLastPromotion', 'YearsWithCurrManager', 'EmpHourlyRate','EmpJobRole_Developer',\n",
    "    'EmpWorkLifeBalance', 'OverTime', 'EmpJobRole_Technical Lead'\n",
    "]\n",
    "\n",
    "# Calculate the mean vector and covariance matrix (for selected numeric features)\n",
    "mean_vector = data[numeric_cols].mean().values\n",
    "cov_matrix = np.cov(data[numeric_cols].values, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "# Function to compute Mahalanobis distance for each row\n",
    "def mahalanobis_distance(row):\n",
    "    diff = row[numeric_cols].values - mean_vector\n",
    "    return mahalanobis(row[numeric_cols].values, mean_vector, inv_cov_matrix)\n",
    "\n",
    "# Compute distances\n",
    "data['Mahalanobis_Dist'] = data.apply(mahalanobis_distance, axis=1)\n",
    "\n",
    "# Determine threshold based on chi-square distribution (degrees of freedom = number of numeric_cols)\n",
    "# For a significance level (e.g., 0.99), points with distance^2 greater than chi2.ppf(0.99, df)\n",
    "threshold = np.sqrt(chi2.ppf(0.99, df=len(numeric_cols)))\n",
    "print(\"Mahalanobis Distance Threshold:\", threshold)\n",
    "\n",
    "# Flag outliers: rows with Mahalanobis distance greater than threshold\n",
    "data['Outlier_Flag'] = data['Mahalanobis_Dist'] > threshold\n",
    "\n",
    "# Optional: Review the number and sample of flagged outliers\n",
    "print(\"Total Outliers Flagged:\", data['Outlier_Flag'].sum())\n",
    "print(\"Sample Outlier Rows:\\n\", data[data['Outlier_Flag']].head())\n",
    "\n",
    "# Decision Point: Remove outliers or keep them?\n",
    "# I have set data_clean = data to keep all rows.\n",
    "data_with_outliers = data.copy()\n",
    "# Uncomment the following line if you decide to remove the outliers.\n",
    "data_without_outliers = data[~data['Outlier_Flag']].copy()\n",
    "\n",
    "\n",
    "##############################\n",
    "# STEP 1: Correlation Analysis with the Target\n",
    "##############################\n",
    "# We want to know which features are most correlated with our target column: PerformanceRating.\n",
    "\n",
    "# Calculate correlation on the cleaned dataset (or on data if outliers are kept)\n",
    "correlation_matrix = data_with_outliers.corr()\n",
    "target_corr = correlation_matrix[\"PerformanceRating\"].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation of Features with PerformanceRating:\\n\", target_corr)\n",
    "\n",
    "# Optionally, select features with absolute correlation above a threshold (e.g., 0.3)\n",
    "abs_corr = correlation_matrix[\"PerformanceRating\"].abs()\n",
    "selected_features = abs_corr[abs_corr > 0.3].index.tolist()\n",
    "if \"PerformanceRating\" in selected_features:\n",
    "    selected_features.remove(\"PerformanceRating\")\n",
    "print(\"\\nSelected features (|corr| > 0.3):\", selected_features)\n",
    "\n",
    "#note Mahalanobis Distance is not considered in the selected features as it is used for outlier detection. and it is computationally expensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate correlation on the cleaned dataset (or on data if outliers are kept)\n",
    "correlation_matrix = data_without_outliers.corr()\n",
    "target_corr = correlation_matrix[\"PerformanceRating\"].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation of Features with PerformanceRating:\\n\", target_corr)\n",
    "\n",
    "# Optionally, select features with absolute correlation above a threshold (e.g., 0.3)\n",
    "abs_corr = correlation_matrix[\"PerformanceRating\"].abs()\n",
    "selected_features = abs_corr[abs_corr > 0.3].index.tolist()\n",
    "if \"PerformanceRating\" in selected_features:\n",
    "    selected_features.remove(\"PerformanceRating\")\n",
    "print(\"\\nSelected features (|corr| > 0.3):\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for column in data_without_outliers.columns:\n",
    "    unique_values = data_without_outliers[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_clean=data_without_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
